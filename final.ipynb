{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\miniconda3\\envs\\forPyTorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import speech_recognition as sr \n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from pydub import AudioSegment  \n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import moviepy.editor as mp \n",
    "from moviepy.editor import VideoFileClip  \n",
    "from googletrans import Translator\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [18/Mar/2024 15:21:46] \"POST /api/convert-mp3-to-text HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Mar/2024 15:21:49] \"GET /api/translate_toen/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Mar/2024 15:21:50] \"POST /api/analyze-audio/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "127.0.0.1 - - [18/Mar/2024 15:22:12] \"POST /api/findtopic/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Your max_length is set to 64, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in summarization using gensim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Mar/2024 15:22:27] \"POST /api/findSummary/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "127.0.0.1 - - [18/Mar/2024 15:22:29] \"POST /api/sentiment/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Mar/2024 15:22:44] \"GET /api/translate_totr/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Mar/2024 15:22:47] \"GET /api/translate_tohi/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Mar/2024 15:22:53] \"GET /api/translate_toen/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [18/Mar/2024 15:22:56] \"GET /api/translate_toar/American%20accent%20in%2010%20seconds%20is%20it%20is%20saying%20is%20he%20nice%20say%20easy%20nice%20easy%20nice%20easy%20nice%20nice HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app, origins=\"http://localhost:3000\", supports_credentials=True)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@app.route('/api/analyze-audio/<string:text>', methods=['POST'])\n",
    "def analyze_text(text):\n",
    "    if len(text) == 0:\n",
    "        return jsonify({'error': 'No text provided'}), 400\n",
    "    \n",
    "    doc = nlp(text) \n",
    "    person_count = len([ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]) \n",
    "    topics = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    name = request.form['name']\n",
    "    file = request.files['file']\n",
    "\n",
    "    directory_path = os.path.join('uploads', name) \n",
    "    filesname, extension = os.path.splitext(file.filename)\n",
    "    text_path = os.path.join(directory_path, \"{}.txt\".format(filesname))  \n",
    "\n",
    "    with open(text_path, 'a') as fil:\n",
    "        fil.write(\"\\nPerson Count: {}\\n\".format(str(person_count + 1)))\n",
    "   \n",
    "    return jsonify({'person_count' : person_count + 1 , 'topic' : topics[0], 'topic2' : topics[1]})\n",
    "\n",
    "@app.route('/api/convert-video-to-mp3', methods=['POST'])\n",
    "def convert_video_to_mp3():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file provided'}), 400\n",
    "\n",
    "    video_file = request.files['file']\n",
    "    name = request.form['name']\n",
    "\n",
    "    if video_file.filename == '':\n",
    "        return jsonify({'error': 'No selected file'}), 400\n",
    "\n",
    "    if video_file: \n",
    "        try:  \n",
    "            print(video_file.filename)\n",
    "            directory_path = os.path.join('uploads', name)\n",
    "            os.makedirs(directory_path, exist_ok=True) \n",
    "            video_path = os.path.join(directory_path, video_file.filename) \n",
    "            filesname, extension = os.path.splitext(video_file.filename)\n",
    "            audio_path = os.path.join(directory_path, \"{}.mp3\".format(filesname)) \n",
    "            video_file.save(video_path)\n",
    "            video_clip = VideoFileClip(video_path)\n",
    "            audio_clip = video_clip.audio\n",
    "            audio_clip.write_audiofile(audio_path)\n",
    "            audio_clip.close()\n",
    "            video_clip.close()\n",
    "\n",
    "            return jsonify({'message': 'Video converted to MP3 successfully', 'audio_file': audio_path}), 200\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': 'Conversion error', 'details': str(e)}), 500\n",
    "\n",
    "def get_audio_length(audio_filename):\n",
    "    audio = AudioSegment.from_file(audio_filename)\n",
    "    return len(audio) / 1000  # Return length in seconds    \n",
    "\n",
    "def split_audio(input_audio, output_dir, segment_length_ms=60000):\n",
    "    print(\"Proccessing Input........\") \n",
    "    audio = AudioSegment.from_file(input_audio) \n",
    "    total_length_ms = len(audio) \n",
    "    num_segments = total_length_ms // segment_length_ms \n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start_time = i * segment_length_ms\n",
    "        end_time = (i + 1) * segment_length_ms\n",
    "        segment = audio[start_time:end_time] \n",
    "        segment.export(os.path.join(output_dir, f\"segment_{i}.wav\"), format=\"wav\")\n",
    "        \n",
    "    if total_length_ms % segment_length_ms > 0:\n",
    "        start_time = num_segments * segment_length_ms\n",
    "        end_time = total_length_ms\n",
    "        last_segment = audio[start_time:end_time]\n",
    "        last_segment.export(os.path.join(output_dir, f\"segment_{num_segments}.wav\"), format=\"wav\")\n",
    "\n",
    "def transcribe_audio_segments(segment_dir, lan):\n",
    "    print(\"Converting to text........\")\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    transcribed_texts = []\n",
    "    \n",
    "    for filename in os.listdir(segment_dir): \n",
    "        audio_file = os.path.join(segment_dir, filename)\n",
    "        \n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            audio_text = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio_text, language=lan)\n",
    "            transcribed_texts.append(text) \n",
    "\n",
    "    return transcribed_texts\n",
    "\n",
    "def join_transcribed_texts(texts):\n",
    "    return \" \".join(texts)\n",
    "\n",
    "def convert_mp3_to_wav(mp3_filename):\n",
    "    wav_filename = mp3_filename.replace('.mp3', '.wav')\n",
    "    audio = AudioSegment.from_mp3(mp3_filename)\n",
    "    audio.export(wav_filename, format=\"wav\")\n",
    "    return wav_filename\n",
    "\n",
    "@app.route('/api/convert-mp3-to-text', methods=['POST'])\n",
    "def convert_mp3_to_text():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file part'}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "    language = request.form['language']\n",
    "    name = request.form['name']\n",
    "    \n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No selected file'}), 400\n",
    "\n",
    "    if file:  \n",
    "        directory_path = os.path.join('uploads', name)\n",
    "        os.makedirs(directory_path, exist_ok=True) \n",
    "        mp3_filename = os.path.join(directory_path, file.filename)\n",
    "\n",
    "        file.save(mp3_filename) \n",
    "        segment_dir = 'segments'  \n",
    "        os.makedirs(segment_dir, exist_ok=True)\n",
    "        audio_length = get_audio_length(mp3_filename)\n",
    "\n",
    "        if audio_length > 60:\n",
    "            split_audio(mp3_filename, segment_dir, segment_length_ms=60000)\n",
    "            transcribed_texts = transcribe_audio_segments(segment_dir, language) \n",
    "            joined_text = join_transcribed_texts(transcribed_texts)\n",
    "        else:\n",
    "            recognizer = sr.Recognizer()\n",
    "\n",
    "            wav_filename = convert_mp3_to_wav(mp3_filename)\n",
    "\n",
    "            with sr.AudioFile(wav_filename) as source:\n",
    "                audio_text = recognizer.record(source)\n",
    "                text = recognizer.recognize_google(audio_text, language=language)\n",
    "                joined_text = text\n",
    "\n",
    "        filesname, extension = os.path.splitext(file.filename)\n",
    "        text_path = os.path.join(directory_path, \"{}.txt\".format(filesname)) \n",
    "\n",
    "        with open(text_path, 'w') as fil:\n",
    "            fil.write(\"Converted Text: \" + joined_text + \"\\n\")\n",
    "\n",
    "        return jsonify({'text': joined_text})\n",
    "\n",
    "\n",
    "@app.route('/api/convert-mp4-to-text', methods=['POST'])\n",
    "def convert_mp4_to_text():\n",
    "    language = request.form['language']\n",
    "    file = request.files['file']\n",
    "    name = request.form['name']\n",
    "\n",
    "    filesname, extension = os.path.splitext(file.filename)\n",
    "\n",
    "    file = os.path.abspath('./uploads/{}/{}.mp3'.format(name, filesname)) \n",
    "\n",
    "    if file: \n",
    "        mp3_filename = file\n",
    "        wav_filename = mp3_filename.replace('.mp3', '.wav') \n",
    "        AudioSegment.from_mp3(mp3_filename).export(wav_filename, format=\"wav\")\n",
    "\n",
    "    segment_dir = 'segments'\n",
    "    try:\n",
    "        if not os.path.exists(segment_dir):\n",
    "            os.mkdir(segment_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory: {e}\")\n",
    "\n",
    "    audio_length = get_audio_length(mp3_filename)\n",
    "\n",
    "    if audio_length > 60: \n",
    "        split_audio(wav_filename, segment_dir, segment_length_ms=60000)  \n",
    "        transcribed_texts = transcribe_audio_segments(segment_dir,lan=language)  \n",
    "        joined_text = join_transcribed_texts(transcribed_texts) \n",
    "    else:\n",
    "        recognizer = sr.Recognizer() \n",
    "        \n",
    "        with sr.AudioFile(wav_filename) as source:\n",
    "            audio_text = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio_text, language=language)\n",
    "            joined_text = text\n",
    "\n",
    "    directory_path = os.path.join('uploads', name)\n",
    "    text_path = os.path.join(directory_path, \"{}.txt\".format(filesname)) \n",
    "\n",
    "    with open(text_path, 'w') as fil:\n",
    "        fil.write(\"Converted Text: \" + joined_text + \"\\n\")\n",
    "\n",
    "    return jsonify({'text': joined_text})\n",
    "\n",
    "\n",
    "@app.route('/api/translate_toar/<string:text>', methods=['GET'], endpoint='translate_to_ar')\n",
    "def translate_to_ar(text):    \n",
    "    translator = Translator() \n",
    "    arabic_translation = translator.translate(text,  src='auto', dest='ar').text\n",
    "    return jsonify({'translated_txt': arabic_translation})\n",
    "\n",
    "@app.route('/api/translate_totr/<string:text>', methods=['GET'], endpoint='translate_to_tr')\n",
    "def translate_to_tr(text):  \n",
    "    translator = Translator()    \n",
    "\n",
    "    turkish_translation = translator.translate(text, src='auto', dest='tr').text\n",
    "    return jsonify({'translated_txt': turkish_translation})\n",
    "\n",
    "@app.route('/api/translate_toen/<string:text>', methods=['GET'], endpoint='translate_to_en')\n",
    "def translate_to_en(text):  \n",
    "    translator = Translator()    \n",
    "\n",
    "    english_translation = translator.translate(text, src='auto', dest='en').text\n",
    "    return jsonify({'translated_txt': english_translation})\n",
    "\n",
    "@app.route('/api/translate_tohi/<string:text>', methods=['GET'], endpoint='translate_to_hi')\n",
    "def translate_to_hi(text):  \n",
    "    translator = Translator()    \n",
    "\n",
    "    hindi_translation = translator.translate(text, src='auto', dest='hi').text\n",
    "    return jsonify({'translated_txt': hindi_translation})\n",
    "\n",
    "def add_newlines_every_n_words(input_string, n=10):\n",
    "    words = input_string.split()\n",
    "    output_string = ''\n",
    "    for i, word in enumerate(words):\n",
    "        if i > 0 and i % n == 0:\n",
    "            output_string += '\\n'\n",
    "        output_string += word + ' '\n",
    "    return output_string.strip()\n",
    "\n",
    "@app.route('/api/findtopic/<string:text>', methods=['POST'])\n",
    "def topic_finder(text):\n",
    "    name = request.form['name']\n",
    "    file = request.files['file']\n",
    "\n",
    "    pipe = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
    "    pipe2 = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
    "\n",
    "    topic = pipe(text)  \n",
    "    topic = topic[0]['label']\n",
    "\n",
    "    topic2 = pipe2(text, max_length = 20)\n",
    "    topic2 = topic2[0]['summary_text']\n",
    "\n",
    "    directory_path = os.path.join('uploads', name) \n",
    "    filesname, extension = os.path.splitext(file.filename)\n",
    "    text_path = os.path.join(directory_path, \"{}.txt\".format(filesname))  \n",
    "\n",
    "    with open(text_path, 'a') as fil:\n",
    "        fil.write(\"\\nTopic: {}\\n\".format(topic2))\n",
    "    \n",
    "    return jsonify({'topic': topic, 'topic2': topic2})\n",
    "\n",
    "@app.route('/api/findSummary/<string:text>', methods=['POST'])\n",
    "def summary_find(text):\n",
    "    pipe = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
    "    \n",
    "    temp = add_newlines_every_n_words(text, 30)\n",
    "    try: \n",
    "        summary = summarize(temp)\n",
    "        print(\"len: \", len(summary))\n",
    "    except:\n",
    "        print(\"Error in summarization using gensim\") \n",
    "        summary = \"\"\n",
    "\n",
    "    if len(summary) == 0:\n",
    "        output = pipe(text) \n",
    "        summary = output[0]['summary_text']\n",
    "             \n",
    "    translator = Translator() \n",
    "    arabic_summary = translator.translate(summary, src='en', dest='ar').text\n",
    "    turkish_summary = translator.translate(summary, src='en', dest='tr').text\n",
    "\n",
    "    name = request.form['name']\n",
    "    file = request.files['file']\n",
    "\n",
    "    directory_path = os.path.join('uploads', name) \n",
    "    filesname, extension = os.path.splitext(file.filename)\n",
    "    text_path = os.path.join(directory_path, \"{}.txt\".format(filesname))  \n",
    "\n",
    "    with open(text_path, 'a') as fil:\n",
    "        fil.write(\"\\nSummary: {}\\n\".format(str(summary)))\n",
    "\n",
    "    return jsonify({'summary_en': str(summary), 'summary_ar': arabic_summary, 'summary_tr': turkish_summary})\n",
    "\n",
    "\n",
    "@app.route('/api/sentiment/<string:text>', methods=['POST'])\n",
    "def sentiment(text):\n",
    "    name = request.form['name']\n",
    "    file = request.files['file']\n",
    "\n",
    "    nltk.download('vader_lexicon')\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    positive_percent = round(scores['pos'] * 100, 2)\n",
    "    negative_percent = round(scores['neg'] * 100, 2)\n",
    "\n",
    "    total = positive_percent + negative_percent\n",
    "    fin_pos = round((positive_percent/total)*100, 2)\n",
    "    fin_neg = round((negative_percent/total)*100, 2) \n",
    "\n",
    "    directory_path = os.path.join('uploads', name) \n",
    "    filesname, extension = os.path.splitext(file.filename)\n",
    "    text_path = os.path.join(directory_path, \"{}.txt\".format(filesname))  \n",
    "\n",
    "    with open(text_path, 'a') as fil:\n",
    "        fil.write(\"\\nPositive Sentiment: {}% \\nNegative Sentiment: {}%\".format(fin_pos, fin_neg))\n",
    "\n",
    "    return jsonify({'positive': fin_pos, 'negative': fin_neg})\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
